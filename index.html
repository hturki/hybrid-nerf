
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="StyleSheet" href="css/projects.css" type="text/css" media="all">
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
  <title>HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces</title>
     <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
        if you update and want to force Facebook to re-scrape. -->
    <meta property="og:title" content="HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces"/>
    <meta property="og:type" content="article" />
    <meta property="og:description"
          content="Neural radiance fields provide state-of-the-art view synthesis quality but tend to be slow to render. One reason is that they make use of volume rendering, thus requiring many samples (and model queries) per ray at render time. Although this representation is flexible and easy to optimize, most real-world objects can be modeled more efficiently with surfaces instead of volumes, requiring far fewer samples per ray. This observation has spurred considerable progress in surface representations such as signed distance functions, but these may struggle to model semi-opaque and thin structures. We propose a method, HybridNeRF, that leverages the strengths of both representations by rendering most objects as surfaces while modeling the (typically) small fraction of challenging regions volumetrically. We evaluate HybridNeRF against the challenging Eyeful Tower dataset along with other commonly used view synthesis datasets. When comparing to state-of-the-art baselines, including recent rasterization-based approaches, we improve error rates by 15–30% while achieving real-time framerates (at least 36 FPS) for virtual- reality resolutions (2K×2K)."/>
    <meta property="og:image" content="https://haithemturki.com/hybrid-nerf/images/teaser.gif">
    <meta property="og:image:width" content="344" /> 
    <meta property="og:image:height" content="282" />
    <meta property="og:url" content="https://haithemturki.com/hybrid-nerf">
    <meta name="twitter:card" content="summary_large_image">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-QFX77EN7S4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-QFX77EN7S4');
</script>
</head>

<body>

  <div class="content content-title" style="text-align: center">
    <h1>HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces</h1>
  </div>

<div id="authors">
    <h3>
        <span><a href="https://haithemturki.com">Haithem Turki</a><sup>1,2</sup></span>
        <span><a href="https://notes.vasuagrawal.com/">Vasu Agrawal</a><sup>1</sup></span>
        <span><a href="https://scholar.google.com/citations?hl=de&user=484sccEAAAAJ">Samuel Rota Bulò</a><sup>1</sup></span>
        <span><a href="https://scholar.google.com/citations?user=vW1gaVEAAAAJ">Lorenzo Porzi</a><sup>1</sup></span>
        <br/>
        <span><a href="https://scholar.google.com/citations?user=CxbDDRMAAAAJ&hl=en">Peter Kontschieder</a><sup>1</sup></span>
        <span><a href="http://www.cs.cmu.edu/~deva">Deva Ramanan</a><sup>2</sup></span>
        <span><a href="https://zollhoefer.com">Michael Zollhöfer</a><sup>1</sup></span>
        <span><a href="https://richardt.name">Christian Richardt</a><sup>1</sup></span>
    </h3>

    <h3>
        <span><sup>1&nbsp;</sup>Meta Reality Labs</span>
        <span><sup>2&nbsp;</sup>Carnegie Mellon University</span>
    <h3>

    <h3 id="menu"><a href='./resources/paper.pdf'>Paper</a></h3>

</div>

  <div class="content">
    <!-- <img src="resources/clusters.jpg" style="width: 80%; margin: auto; display: block; padding-bottom: 20px"> -->
    <div style="text-align: center; padding-bottom:20px">
        <h3>Abstract</h3>
    </div>
    <figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px; text-align:center">
      <figcaption>
        Neural radiance fields provide state-of-the-art view synthesis quality but tend to be slow to render. One reason is that they make use of volume rendering, thus requiring many samples (and model queries) per ray at render time. Although this representation is flexible and easy to optimize, most real-world objects can be modeled more efficiently with surfaces instead of volumes, requiring far fewer samples per ray. This observation has spurred considerable progress in surface representations such as signed distance functions, but these may struggle to model semi-opaque and thin structures. We propose a method, HybridNeRF, that leverages the strengths of both representations by rendering most objects as surfaces while modeling the (typically) small fraction of challenging regions volumetrically. We evaluate HybridNeRF against the challenging <a href="https://github.com/facebookresearch/EyefulTower">Eyeful Tower</a> dataset along with other commonly used view synthesis datasets. When comparing to state-of-the-art baselines, including recent rasterization-based approaches, we improve error rates by 15–30% while achieving real-time framerates (at least 36 FPS) for virtual- reality resolutions (2K×2K).
      </figcaption>
    </figure>
  </div>

  <div class="content">
    <div style="text-align: center; padding-bottom:20px">
        <h3>Overview</h3>
    </div>
    <figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
        <video width="90%" controls style="margin-left: auto; margin-right: auto; display: block;" poster="./images/overview.jpg">
            <source src="./vids/overview.mp4" type="video/mp4">
        </video>
    </figure>
  </div>

  <div class="content">
    <div style="text-align: center; padding-bottom:20px">
        <h3>Eyeful Tower</h3>
    </div>
    <figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
        <figcaption style="margin-bottom: 20px">
            We render novel views from the Workshop scene in the <a href="https://github.com/facebookresearch/EyefulTower">Eyeful Tower</a> dataset. Since we train with HDR images, we are able to render the scene at different exposures.
        </figcaption>
        <video width="90%" controls style="margin-left: auto; margin-right: auto; display: block;" poster="./images/workshop-overview.jpg">
            <source src="./vids/workshop-overview.mp4" type="video/mp4">
        </video>
    </figure>
  </div>

<div class="content">
    <div style="text-align: center; padding-bottom:20px">
        <h3>Eyeful Tower Comparisons</h3>
        <button class="selected-scene" id="workshop">Workshop</button><button id="office_view1">Office View 1</button><button id="office1b">Office 1B</button>
    </div>
    <figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
        <figcaption style="margin-bottom: 20px">
            We compare HybridNeRF to prior work (FPS shown for 2Kx2K rendering on a single Nvidia 4090 GPU). <a href="https://creiser.github.io/merf/">MERF</a>, <a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">3DGS</a>, and <a href="https://lioryariv.github.io/volsdf/">VolSDF<sup>*</sup></a> (our implementation using iNGP accleration primitives) do not support training models in HDR and therefore do not support exposure adjustments.
        </figcaption>
        <figcaption style="margin-bottom: 20px" id="eyeful-comparison">
            Our quality is slightly better than <a href="https://vr-nerf.github.io/">VR-NeRF</a> while rendering over 10x faster. Our results are significantly better than faster rendering approaches, especially when modeling specular effects.
        </figcaption>
        <video width="90%" controls style="margin-left: auto; margin-right: auto; display: block;" id="eyeful-video">
            <source src="./vids/workshop.mp4" type="video/mp4" poster="./images/workshop.jpg">
        </video>
    </figure>
</div>

<div class="content">
    <div style="text-align: center; padding-bottom:20px">
        <h3>ScanNet++ Comparisons</h3>
        <button class="selected-scene" id="98b4ec142f">98b4ec142f</button><button id="b20a261fdf">b20a261fdf</button>
    </div>
    <figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
        <figcaption style="margin-bottom: 20px">
            We evaluate <a href="https://cy94.github.io/scannetpp/">ScanNet++</a> as another dataset built from high-resolution captures of indoor scenes. <a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">3DGS</a> struggles with reflections and far-field content. Our method performs the best overall while exceeding the 36 FPS target frame rate for VR.
        </figcaption>
        <video width="90%" controls style="margin-left: auto; margin-right: auto; display: block;" id="snpp-video">
            <source src="./vids/snpp_98b4ec142f.mp4" type="video/mp4" poster="./images/snpp_98b4ec142f.jpg">
        </video>
    </figure>
</div>

<div class="content">
    <div style="text-align: center; padding-bottom:20px">
        <h3>MipNeRF 360 Comparisons</h3>
        <button class="selected-scene" id="stump">Stump</button><button id="treehill">Treehill</button><button id="kitchen">Kitchen</button>
    </div>
    <figure style="font-style: italic; font-weight: normal; margin: 0px; padding: 0px; border: 0px">
        <figcaption style="margin-bottom: 20px">
            We also evaluate HybridNeRF against the <a href="https://jonbarron.info/mipnerf360/">Mip-NeRF 360</a> dataset, where our approach performs comparably to other state of the art methods. We render novel camera trajectories that show how volume-based methods such as <a href="https://nvlabs.github.io/instant-ngp/">iNGP</a> "cheat" when modeling apparent surfaces. Our method renders faster and generates more plausible surface geometry.
        </figcaption>
        <video width="90%" controls style="margin-left: auto; margin-right: auto; display: block;" id="m360-video">
            <source src="./vids/stump.mp4" type="video/mp4" poster="./images/stump.jpg">
        </video>
    </figure>
</div>

<div class="content">
    <div style="text-align: center; padding-bottom:20px">
        <h3>Citation</h3>
    </div>
    <figure style="font-weight: normal; text-align:center">
        <pre><code>@misc{turki2023hybridnerf,
      title={HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces}, 
      author={Haithem Turki and Vasu Agrawal and Samuel Rota Bulò and Lorenzo Porzi and Peter Kontschieder and Deva Ramanan and Michael Zollh\"{o}fer and Christian Richardt},
      year={2023},
      eprint={2312.03160},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</code></pre>
    </figure>
  </div>

<script src="resources/handler.js"></script>

</html>

